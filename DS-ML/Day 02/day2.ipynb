{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Introduction to the Data Science Toolbox</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "* [SciPy](#tbx_scipy) [Math & Stat]\n",
    "* [StatsModels](#tbx_stats) [Math & Stat]\n",
    "* [Numpy](#tbx_numpy) [Math & Stat]\n",
    "* [Pandas](#tbx_pandas) [Data Processing]\n",
    "* [Scikit-learn](#tbx_sklearn) [Machine Learning]\n",
    "* [Keras](#tbx_keras) [Deep Learning]\n",
    "* [Matplotlib, Seaborn](#tbx_vis) [Visualization]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tbx_scipy'></a>\n",
    "## SciPy\n",
    "\n",
    "<img src=\"imgs/scipy.png\" alt=\"drawing\" width=\"200\">\n",
    "\n",
    "**The SciPy ecosystem**\n",
    "\n",
    "Scientific computing in Python builds upon a small core of packages:\n",
    "\n",
    "**Python**, a general purpose programming language. It is interpreted and dynamically typed and is very suited for interactive work and quick prototyping, while being powerful enough to write large applications in.\n",
    "\n",
    "**NumPy**, the fundamental package for numerical computation. It defines the numerical array and matrix types and basic operations on them.\n",
    "\n",
    "The **SciPy** library, a collection of numerical algorithms and domain-specific toolboxes, including signal processing, optimization, statistics and much more.\n",
    "\n",
    "**Matplotlib**, a mature and popular plotting package, that provides publication-quality 2D plotting as well as rudimentary 3D plotting\n",
    "\n",
    "* Website: https://www.scipy.org/\n",
    "* Scipy Lecture Notes: https://scipy-lectures.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tbx_numpy'></a>\n",
    "## Numpy\n",
    "\n",
    "<img src=\"imgs/numpy.png\" alt=\"drawing\" width=\"400\">\n",
    "\n",
    "NumPy is the fundamental package for scientific computing with Python. It contains among other things:\n",
    "\n",
    "* a powerful N-dimensional array object\n",
    "* sophisticated (broadcasting) functions\n",
    "* tools for integrating C/C++ and Fortran code\n",
    "* useful linear algebra, Fourier transform, and random number capabilities\n",
    "\n",
    "Besides its obvious scientific uses, NumPy can also be used as an efficient multi-dimensional container of generic data. Arbitrary data-types can be defined. This allows NumPy to seamlessly and speedily integrate with a wide variety of databases.\n",
    "\n",
    "* Website: https://www.numpy.org/\n",
    "* Numpy Tutorial: http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tbx_pandas'></a>\n",
    "## Pandas\n",
    "\n",
    "<img src=\"imgs/pandas.png\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "**pandas** is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n",
    "\n",
    "* Website: https://pandas.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tbx_sklearn'></a>\n",
    "## Scikit-learn\n",
    "\n",
    "<img src=\"imgs/sklearn.png\" alt=\"drawing\" width=\"300\">\n",
    "\n",
    "Scikit-learn enables you to do Machine Learning in Python.\n",
    "\n",
    "* Simple and efficient tools for data mining and data analysis\n",
    "* Accessible to everybody, and reusable in various contexts\n",
    "* Built on NumPy, SciPy, and matplotlib\n",
    "* Open source, commercially usable - BSD license\n",
    "\n",
    "Website: https://scikit-learn.org/stable/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tbx_keras'></a>\n",
    "## Keras\n",
    "\n",
    "<img src=\"imgs/keras.png\" alt=\"drawing\" width=\"400\">\n",
    "\n",
    "**Keras** is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research.\n",
    "\n",
    "Use Keras if you need a deep learning library that:\n",
    "\n",
    "* Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility).\n",
    "* Supports both convolutional networks and recurrent networks, as well as combinations of the two.\n",
    "* Runs seamlessly on CPU and GPU.\n",
    "\n",
    "Website: https://keras.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='tbx_vis'></a>\n",
    "## Matplotlib, Seaborn\n",
    "\n",
    "<img src=\"imgs/matplotlib.png\" alt=\"drawing\" width=\"400\">\n",
    "\n",
    "**Matplotlib** is a Python 2D plotting library which produces publication quality figures in a variety of hardcopy formats and interactive environments across platforms. Matplotlib can be used in Python scripts, the Python and IPython shells, the Jupyter notebook, web application servers, and four graphical user interface toolkits.\n",
    "\n",
    "**Seaborn** is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "* Website: https://matplotlib.org/\n",
    "* Website: https://seaborn.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Diving into Data Science</h1></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "* [Data Science Pipeline](#pipeline)\n",
    "* [Data Collection](#collection)\n",
    "* [Data Exploration](#exploration)\n",
    "* [Data Preprocessing](#preprocessing)\n",
    "* [Data Modeling](#modeling)\n",
    "* [Model Validation](#validation)\n",
    "* [Communication](#communication)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pipeline'></a>\n",
    "## Data Science Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/data_pipeline.png\" alt=\"drawing\" width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='collection'></a>\n",
    "## 1. Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h3>90% OF THE WORLDâ€™S DATA WAS CREATED IN THE LAST 2 YEARS</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** Data collection is a process of collecting information from all the relevant sources to find answers to the research problem, test the hypothesis and evaluate the outcomes.\n",
    "\n",
    "Information you gather can come from a range of sources. Likewise, there are a variety of techniques to use when gathering primary data. Listed below are some of the most common data collection techniques. [**Source: <a href=\"https://cyfar.org/data-collection-techniques\">cyfar</a>**]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:navy;display:inline;\">Technique: <span style=\"color:black\">Interview</span></h3> \n",
    "\n",
    "**Key Facts:**\n",
    "* Interviews can be conducted in person or over the telephone\n",
    "* Interviews can be done formally (structured), semi-structured, or informally\n",
    "* Questions should be focused, clear, and encourage open-ended responses\n",
    "* Interviews are mainly qualitative in nature\n",
    "\n",
    "**Example:** One-on-one conversation with parent of at-risk youth who can help you understand the issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:navy;display:inline;\">Technique: <span style=\"color:black\"> Questionnaires and Surveys</span></h3> \n",
    "\n",
    "**Key Facts:**\n",
    "* Responses can be analyzed with quantitative methods by assigning numerical values to Likert-type scales\n",
    "* Results are generally easier (than qualitative techniques) to analyze\n",
    "* Pretest/Posttest can be compared and analyzed\n",
    "\n",
    "**Example:** Results of a satisfaction survey or opinion survey."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:navy;display:inline;\">Technique: <span style=\"color:black\"> Observations</span></h3> \n",
    "\n",
    "**Key Facts:**\n",
    "* Allows for the study of the dynamics of a situation, frequency counts of target behaviors, or other behaviors as indicated by needs of the evaluation.\n",
    "* Good source for providing additional information about a particular group, can use video to provide documentation.\n",
    "* Can produce qualitative (e.g., narrative data) and quantitative data (e.g., frequency counts, mean length of interactions, and instructional time).\n",
    "\n",
    "**Example:** Site visits to an after-school program to document the interaction between youth and staff within the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:navy;display:inline;\">Technique: <span style=\"color:black\"> Focus Groups</span></h3> \n",
    "\n",
    "**Key Facts:**\n",
    "* A facilitated group interview with individuals that have something in common.\n",
    "* Gathers information about combined perspectives and opinions.\n",
    "* Responses are often coded into categories and analyzed thematically.\n",
    "\n",
    "**Example:** A group of parents of teenagers in an after-school program are invited to informally discuss programs that might benefit and help their children succeed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:navy;display:inline;\">Technique: <span style=\"color:black\"> Documents and Records</span></h3> \n",
    "\n",
    "**Key Facts:**\n",
    "* Consists of examining existing data in the form of databases, meeting minutes, reports, attendance logs, financial records, medical records, newsletters, PDFs etc.\n",
    "* This can be an inexpensive way to gather information but may be an incomplete data source\n",
    "\n",
    "**Example:** To understand the primary reasons students miss school, records on student absences are collected and analyzed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:navy;display:inline;\">Technique: <span style=\"color:black\"> Web Scraping and Web APIs</span></h3> \n",
    "\n",
    "**Key Facts:**\n",
    "* Involves scraping and extracting data (text, tables, images etc) from different websites. Using APIs like Facebook Graph API, Twitter API etc.\n",
    "* This method requires web programming knowledge.\n",
    "\n",
    "**Example:** To understand the user sentiment of Facebook online shop pages, comments are collected from the posts. \n",
    "\n",
    "**Selinium and Beautifulsoup is two popular Data Scrapping tool**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:navy;display:inline;\">Technique: <span style=\"color:black\"> Sensors, Mobile and IoT Device</span></h3> \n",
    "\n",
    "**Key Facts:**\n",
    "* Collects data from sensors, EEG Signals, MRI, Mobile SMS, Hand-badges, IoT Devices.\n",
    "* Data collected from these sources are often noisy and sometimes difficult import/prepare into readable format.\n",
    "\n",
    "**Example:** To classify eye state EEG Signal data are collected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opensource Datasets\n",
    "\n",
    "There are lots of opensourced datasets in the web. \n",
    "The most popular 2 data sources are:\n",
    "\n",
    "1. **UCI Machine Learning Repository**: https://archive.ics.uci.edu/ml/index.php\n",
    "2. **Kaggle**: https://www.kaggle.com/datasets\n",
    "\n",
    "Others:\n",
    "* **We have a data source from the govt.:** http://data.gov.bd/\n",
    "* **Github:** https://github.com/awesomedata/awesome-public-datasets\n",
    "* **Google Data Search:** https://toolbox.google.com/datasetsearch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='exploration'></a>\n",
    "## 2. Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition:** Data exploration aka EDA (Exploratory Data Analysis) is the initial step in data analysis, where a data analyst uses visual exploration to understand a data set uncovering its initial patterns, characteristics, and points of interest.\n",
    "\n",
    "In other words, EDA is used to understand, summarize and analyse the contents of a dataset, usually to investigate a specific question or to prepare for more advanced modeling.\n",
    "\n",
    "Few basic data exploration approaches are discussed below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Analysis (Exploration Phase)\n",
    "\n",
    "In real-world data is often missing. There are few reasons why data goes missing.\n",
    "\n",
    "\n",
    "### Treatment (Preprocessing Phase)\n",
    "* In the first two cases, it is safe to remove the data with missing values depending upon their occurrences.\n",
    "* In the third case removing observations with missing values can produce a bias in the model. So we have to be really careful before removing observations.\n",
    "* Imputing missing values.\n",
    "* Using models that are robust to missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Analysis\n",
    "\n",
    "**Outlier:** An outlier is a data point that differs significantly from other observations.\n",
    "\n",
    "An outlier can cause serious problems in training models.\n",
    "\n",
    "#### Outlier detection (Exploration Phase)\n",
    "* One of the best visualization approaches to use are boxplots for univariate analysis and scatterplots for bi-variate analysis.\n",
    "* Use the interquartile range theory.\n",
    "\n",
    "#### Treatment (Preprocessing Phase)\n",
    "* We can either delete them if they are very few or if not.\n",
    "* We can use a special treatment like triming, imputing them.\n",
    "* Use models that are robust to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class Imbalance Analysis\n",
    "\n",
    "**Definition:** Data are said to suffer the Class Imbalance Problem when the class distributions are highly imbalanced. In this context, classification learning algorithms have low predictive accuracy for the infrequent class.\n",
    "\n",
    "#### Class Imbalance detection (Exploration Phase)\n",
    "* Use Bar Plot or Pie Chart to visualize the distribution of the classes (target variable).\n",
    "* Simly make a frequency table for the classes (target variable).\n",
    "\n",
    "#### Treatment (Preprocessing Phase)\n",
    "* Apply sampling methods like Oversampling or Undersampling.\n",
    "* Apply class weighting while training models.\n",
    "* Use voting or ensemble strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='preprocessing'></a>\n",
    "## 3. Data Preprocessing\n",
    "\n",
    "**Definition:** Data preprocessing is the phase that involves transforming raw data into an understandable format. Real-world data is often incomplete, inconsistent, and/or lacking in certain behaviors or trends, and is likely to contain many errors. Data preprocessing is a proven method of resolving such issues.\n",
    "\n",
    "Data preprocessing is the first (and arguably most important) step toward building a working machine learning model. It's critical! If your data hasn't been cleaned and preprocessed, your model won't work. It's that simple.\n",
    "\n",
    "Data preprocessing is generally thought of as the boring part. However, it makes the difference between the best performing model and the other models.\n",
    "\n",
    "Here are few basic tasks that are involved in the data preprocessing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Noises\n",
    "Real-world data can often contain extra noises. \n",
    "\n",
    "**For example:**\n",
    "* Online survey data may contain special characters.\n",
    "* Missing cell may contain special characters than space or empty cell.\n",
    "* Web scraped data often includes unnecessary HTML tags.\n",
    "* Image, voice and signal data are often noisy and unclear.\n",
    "\n",
    "Such extra noises should be removed at first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Value Treatment\n",
    "Missing value treatment is one of the common scenario in the data preprocessing phase. The detection of missing values is performed in the data exploration phase. After identifying the probable reason behind the missingness, we should choose a missing value treatment strategy and apply to the missing data in this phase. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Treatment\n",
    "\n",
    "Real-world data often contains outliers. Treatment for the outliers are crucial for training a good model; otherwise it may harm the learnability of the models. After detecting the outliers in the exploration phase we need to choose a strategy for handling the extreme values and apply it in this phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Class Treatment\n",
    "\n",
    "This is another problematic scenario in classification tasks. Imbalanced target classes make your model biased. The model tend to learn the most from the majority class and can not learn the patterns for the minority class. The imbalanced class and its ratio should be addressed in the exploration phase. In this preprocessing phase, we should apply a imbalanced class handling strategy to balance the learning of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Encoding\n",
    "\n",
    "Feature encoding simply means converting the categorical features into numerical. No algorithms understands character/text values. We must represent the categorical features in our dataset into a numerical form to feed the data to the model. The most commonly used 2 feature encoding methods are described below.\n",
    "\n",
    "#### Label Encoding\n",
    "Label Encoding is simply encoding/replacing the categorical value with a number that is unique for that particular class.\n",
    "\n",
    "Label Encoding in Python can be achieved using Sklearn/Pandas Library. Sklearn provides a very efficient tool for encoding the levels of categorical features into numeric values. **LabelEncoder** encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier.\n",
    "\n",
    "**For Example:**\n",
    "<img src=\"imgs/label_encoding.png\" alt=\"drawing\" width=\"500\">\n",
    "\n",
    "But depending on the data, label encoding introduces a new problem. For example, we have encoded a set of country names into numerical data. This is actually categorical data and there is no relation, of any kind, between the rows.\n",
    "\n",
    "The problem here is since there are different numbers in the same column, the model will misunderstand the data to be in some kind of order, 0 < 1 <2.\n",
    "\n",
    "The model may derive a correlation like as the country number increases the population increases but this clearly may not be the scenario in some other data or the prediction set. To overcome this problem, we use One Hot Encoding.\n",
    "\n",
    "\n",
    "#### One Hot Encoding\n",
    "\n",
    "One hot encoding takes a column which has categorical data and then splits the column into multiple columns. The numbers are replaced by 1s and 0s, depending on which column has what value.\n",
    "\n",
    "**For Example:**\n",
    "<img src=\"imgs/one_hot_encoding.png\" alt=\"drawing\" width=\"600\">\n",
    "\n",
    "One hot encoding has also problems, it increases the dimentionality of the dataset.\n",
    "\n",
    "#### When to use Lablen Encoding and One Hot Encoding\n",
    "\n",
    "* Use Label Encoding for tree-based algorithms.\n",
    "* Use One Hot Encoding for distance-based algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Scaling \n",
    "Data Scaling aka Feature scaling is a method used to standardize the range of independent variables or features of data.\n",
    "\n",
    "\n",
    "#### Why to scale the features?\n",
    "Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization. For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance.\n",
    "\n",
    "Another reason why feature scaling is applied is that gradient descent converges much faster with feature scaling than without it.\n",
    "\n",
    "The most 3 popular feature scaling methods are described below:\n",
    "\n",
    "#### Rescaling\n",
    "It is commonly known as min-max scaling. It is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [âˆ’1, 1]. Selecting the target range depends on the nature of the data. The general formula is given as:\n",
    "\n",
    "${\\displaystyle x'={\\frac {x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}}$\n",
    "\n",
    "where ${\\displaystyle x}$ is an original value, ${\\displaystyle x'}$ is the rescaled value. \n",
    "\n",
    "\n",
    "#### Mean Normalization\n",
    "The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "Normal distribution (Gaussian distribution), also known as the bell curve, is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean.\n",
    "\n",
    "The general formula is given as:\n",
    "\n",
    "${\\displaystyle x'={\\frac {x-{\\text{mean}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}}}$\n",
    "\n",
    "where ${\\displaystyle x}$ is an original value, ${\\displaystyle x'}$ is the normalized value. \n",
    "\n",
    "\n",
    "#### Standardization\n",
    "Standardization transforms your data such that the resulting distribution has a mean of 0 and a standard deviation of 1.\n",
    "\n",
    "The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation.\n",
    "\n",
    "${\\displaystyle x'={\\frac {x-{\\bar {x}}}{\\sigma }}}$\n",
    "\n",
    "Where $x$ is the original feature vector, ${\\bar{x}={\\text{average}}(x)}$ is the mean of that feature vector, and $\\sigma$ is its standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "**Definition:** Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms perform the best.\n",
    "\n",
    "Feature Engineering is the key to build the best performing models.\n",
    "\n",
    "**For Example:** Suppose you have some items data with **item_id, weight,** and **price**. Now, you can derive a new feature name **price_per_weight** from the **weight** and **price** columns.\n",
    "\n",
    "<img src=\"imgs/feature_engineering.png\" alt=\"drawing\" width=\"600\">\n",
    "\n",
    "**We'll learn the feature engineering techniques in detail in future chapters. Probably on Day 09.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='modeling'></a>\n",
    "## 4. Data Modeling\n",
    "\n",
    "**Definition:** Data Modeling is the process of applying appropriate Machine Learning algorithms to fit or model the data. It is basically the process of training Machine Learning algorithms to learn the underlying patterns from the data.\n",
    "\n",
    "Data Modeling phase involves 2 crucial decisions.\n",
    "\n",
    "\n",
    "### Choose the right algorithm for your data\n",
    "There should be particular reason behind selecting a particular algorithm for a problem. By default, you can use the Scikit-learn's algorithm cheat sheet to choose an algorithm to model your data.\n",
    "\n",
    "<img src=\"imgs/sklearn_algo_selection.png\" alt=\"drawing\" width=\"800\">\n",
    "\n",
    "<div style=\"text-align: right\"> \n",
    "    Source: <a href=\"https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html\">scikit-learn</a>\n",
    "</div>\n",
    "\n",
    "\n",
    "### Tune the best hyperparameters for your algorithm\n",
    "\n",
    "**Hyperparameter Tuning:** Hyperparameter tuning is choosing a set of optimal hyperparameters for a learning algorithm. \n",
    "\n",
    "So what is a **hyperparameter**?\n",
    "\n",
    "**A hyperparameter** is a parameter whose value is set before the learning process begins.\n",
    "\n",
    "**For Example:** Choosing the value of $k$ for the k-nearest neighbors algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='validation'></a>\n",
    "## 5. Model Validation\n",
    "\n",
    "**Definition:** Model Validation is the phase where we use the trained model to make predictions on the unseen data. Then we use some evaluation metrics to evaluate the model performance and spot overfitting or underfitting.\n",
    "\n",
    "This phase involves 3 tasks:\n",
    "\n",
    "### Making predictions\n",
    "* Use the trained model to make predictions on the **test data** (unseen data).\n",
    "\n",
    "### Choose proper evaluation metrics and evaluate model performance\n",
    "* According to the type of problem, we need to choose few evaluation metrics.\n",
    "* For Regression: MSE, MAE, MAPE, R2 etc.\n",
    "* For Classification: Accuracy, Precision, Recall, F1-Score etc.\n",
    "* For Clustering: Mutual Information Score, Homogeneity Score etc.\n",
    "\n",
    "### Spot overfitting/underfitting\n",
    "Simple ways of spotting overfitting/underfitting in four different categories: \n",
    "1. **Underfitting** â€” Validation and training error high. \n",
    "2. **Overfitting** â€” Validation error is high, training error low. \n",
    "3. **Good fit**â€” Validation error low, slightly higher than the training error.\n",
    "4. **Unknown fit** - Validation error low, training error 'high' I say 'unknown' fit because the result is counter intuitive to how machine learning works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='communication'></a>\n",
    "## 6. Communication\n",
    "\n",
    "**Definition:** In Data Science communication involves interpreting the model, explaining why a model made a particular decision, discovering the hidden patterns, making proper documentations and deploying the model to production.\n",
    "\n",
    "#### Interpret your model\n",
    "* Model explainablity is very crucial. We need to disect the logic behind a models prediction.\n",
    "* We may need to explain how the model works to non-technical people.\n",
    "\n",
    "#### Discover the hidden patterns\n",
    "* Visualize the tree models. Extract the rulesets.\n",
    "* Discover the important features.\n",
    "* Extract the important feature coefficients.\n",
    "\n",
    "#### Make documentation\n",
    "* Making documentation of each steps is very important.\n",
    "* It makes our work easy-to-share, readable and understandable.\n",
    "\n",
    "#### Deploy your model to production\n",
    "* Choosing the right platform for deploying model.\n",
    "* Keeping scalability in mind.\n",
    "* Choose fast and scalable APIs.\n",
    "* Monitor and A/B Test your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An interesting stats on the time expenditure of a Data Scientist on different sections of the data science pipeline.\n",
    "\n",
    "<img src=\"imgs/time-doing.jpg\" alt=\"drawing\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5 style=\"color:red;display:inline;\">**We will strictly follow this pipeline in the upcoming practice and capstone projects.</h5>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
